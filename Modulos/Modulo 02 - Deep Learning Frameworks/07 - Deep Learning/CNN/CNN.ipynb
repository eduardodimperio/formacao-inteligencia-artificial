{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# X_train = X_train / 255.\r\n",
    "# X_test = X_test / 255.\r\n",
    "\r\n",
    "X_train = X_train.astype(np.float32)\r\n",
    "X_test = X_test.astype(np.float32)\r\n",
    "\r\n",
    "\r\n",
    "X_train = X_train.reshape([*X_train.shape, 1])\r\n",
    "X_test = X_test.reshape([*X_test.shape, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "learning_rate = 0.001\r\n",
    "training_iters = 1000\r\n",
    "batch_size = 128\r\n",
    "display_step = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "n_input = 784 # MNIST data input (shape: 28*28)\r\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Dropout\r\n",
    "# Aplicamos o dropout para reduzir o overfitting. O dropout vai eliminar algumas unidades (nas camadas ocultas, de entrada e de saída) na rede neural.\r\n",
    "# A decisão sobre qual neurônio será eliminado é randômica e aplicamos uma probabilidade para isso. Esse parâmetro pode ser ajustado para otimizar o desempenho da rede.\r\n",
    "dropout = 0.75 # Dropout, probabilidade para manter unidades"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(X_train.shape)\r\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Variáveis para os pesos e bias\r\n",
    "\r\n",
    "# Pesos\r\n",
    "# Cada neurônio da camada oculta é conectado a um pequeno grupo de tensores de entrada (input) de dimensão 5x5. Com isso, a camada oculta terá um tamanho de 24x24.\r\n",
    "wc1 = tf.Variable(tf.random.normal([5, 5, 1, 32])) # 5x5 conv, 1 input, 32 outputs\r\n",
    "wc2 = tf.Variable(tf.random.normal([5, 5, 32, 64])) # 5x5 conv, 32 inputs, 64 outputs\r\n",
    "wd1 = tf.Variable(tf.random.normal([4*4*64, 1024])) # fully connected, 7*7*64 inputs, 1024 outputs\r\n",
    "wout = tf.Variable(tf.random.normal([1024, n_classes])) # 1024 inputs, 10 outputs (class prediction)\r\n",
    "\r\n",
    "# Bias\r\n",
    "bc1 = tf.Variable(tf.random.normal([32]))\r\n",
    "bc2 = tf.Variable(tf.random.normal([64]))\r\n",
    "bd1 = tf.Variable(tf.random.normal([1024]))\r\n",
    "bout = tf.Variable(tf.random.normal([n_classes]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def conv2d(image, w, b):\r\n",
    "    conv = tf.nn.conv2d(image, filters=w, strides=[1,1,1,1], padding='VALID')\r\n",
    "    conv = tf.nn.bias_add(conv, b)\r\n",
    "    return tf.nn.relu(conv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def max_pool(image, k):\r\n",
    "    return tf.nn.max_pool(image, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def full_connected(input, w, b):\r\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def cross_entropy(y_true, y_pred):\r\n",
    "    y_true = tf.one_hot(y_true, n_classes)\r\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Cria o otimizador usando Adam\r\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\n",
    "dataset = dataset.repeat().shuffle(X_train.shape[0]).batch(batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "for i, (batch_X_train, batch_y_train) in enumerate(dataset.take(1)):\r\n",
    "    conv1 = conv2d(batch_X_train, wc1, bc1)\r\n",
    "    conv1 = max_pool(conv1, k=2)\r\n",
    "    conv1 = tf.nn.dropout(conv1, dropout)\r\n",
    "\r\n",
    "    conv2 = conv2d(conv1, wc2, bc2)\r\n",
    "    conv2 = max_pool(conv2, k=2)\r\n",
    "    conv2 = tf.nn.dropout(conv2, dropout)\r\n",
    "\r\n",
    "    dense1 = tf.reshape(conv2, [-1, wd1.shape[0]])\r\n",
    "    dense1 = full_connected(dense1, wd1, bd1)\r\n",
    "    dense1 = tf.nn.relu(dense1)\r\n",
    "\r\n",
    "    pred = full_connected(dense1, wout, bout)\r\n",
    "    loss = cross_entropy(batch_y_train, pred)\r\n",
    "    \r\n",
    "    # optimizer.minimize(loss, [wout, bout])\r\n",
    "\r\n",
    "    \r\n",
    "    # loss_val = loss(dense1, batch_y_train, wout, bout, n_classes)\r\n",
    "    # print(loss_val)\r\n",
    "    # optimizer.minimize(loss_val, [wout, bout])\r\n",
    "    # with tf.GradientTape() as t:\r\n",
    "    #     gradient = t.gradient(loss_val, [wout, bout, wd1, bd1, wc2, bc2, wc1, bc1])\r\n",
    "    #     print(gradient)\r\n",
    "        # optimizer.apply_gradients(zip(gradient, [wout, bout, wd1, bd1, wc2, bc2, wc1, bc1]))\r\n",
    "\r\n",
    "\r\n",
    "    # Faz uma previsão\r\n",
    "    #y_pred = logistic_regression(batch_X_train, wout, bout)\r\n",
    "\r\n",
    "    # Calcula o erro\r\n",
    "    #loss = cross_entropy(batch_X_train, batch_y_train, n_classes)\r\n",
    "\r\n",
    "    # Calcula a acurácia\r\n",
    "    #acc = accuracy(batch_X_train, batch_y_train)\r\n",
    "\r\n",
    "    # Print\r\n",
    "    #print(\"Número do Batch: %i, Erro do Modelo: %f, Acurácia em Treino: %f\" % (i, loss, acc))\r\n",
    "    \r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c96f4fdc562d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m# optimizer.minimize(loss, [wout, bout])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\IA\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m    374\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[1;32m--> 375\u001b[1;33m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\IA\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[1;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[0;32m    427\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m       \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('IA': conda)"
  },
  "interpreter": {
   "hash": "cd33b1ad81c559069350aab209dba1ac81a1a78823a0538f4b422d164d6af523"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}